from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from pymilvus import Collection
from src.milvus_client import connect_milvus
from src.embedder import Embedder 
from src.search import search_embedding 
from src.final_answer import generate_answer

app = FastAPI(title="RAG API with LLaMA 3.2-1B and Milvus")
## Initialization
# Connect to Milvus and load collection
print(f"Connecting to Milvus...")
connect_milvus()
print(f"Connected to Milvus!")
print(f"Loading Collection...")
collection = Collection("voss_diary")
collection.load()
print(f"Collection loaded!")

# Initialize the Embedder
# This assumes the Embedder class from src.embedder is used directly for embedding in the API
embedder = Embedder(model_name="Snowflake/snowflake-arctic-embed-s")
## API Schemas
class QueryRequest(BaseModel):
    """
    Schema for the incoming API request to query the RAG system.

    Attributes:
        question (str): The question asked by the user.
    """
    question: str

class QueryResponse(BaseModel):
    """
    Schema for the API response from the RAG system.

    Attributes:
        answer (str): The answer generated by the LLaMA model based on the retrieved context.
    """
    answer: str

## API Endpoint
@app.post("/query", response_model=QueryResponse)
def query_rag(request: QueryRequest):
    """
    API endpoint for querying the RAG (Retrieval-Augmented Generation) system.

    This endpoint orchestrates the RAG pipeline:
    1. Embeds the user's question using the `Embedder` model.
    2. Performs a similarity search in the Milvus collection using `search_embedding`
       to retrieve relevant document chunks (context).
    3. Generates a final answer using the `generate_answer` function (LLaMA model)
       based on the original question and the retrieved context.

    Args:
        request (QueryRequest): The incoming request body containing the user's question.

    Returns:
        QueryResponse: The response containing the generated answer.

    Raises:
        HTTPException: If any error occurs during the process (e.g., embedding failure,
                       Milvus search error, LLM inference error).
    """
    try:
        # 1. Embed the query
        # The embedder.embed_texts expects a list, and we want the first (and only) embedding
        query_embedding = embedder.embed_texts([request.question])[0]

        # 2. Search Milvus for relevant context
        # The search_embedding function directly returns the context string
        context = search_embedding(query_embedding)

        # 3. Generate the final answer using LLaMA
        answer = generate_answer(request.question, context)

        return QueryResponse(answer=answer)
    except Exception as e:
        # Catch any exceptions and return an HTTP 500 error
        raise HTTPException(status_code=500, detail=str(e))